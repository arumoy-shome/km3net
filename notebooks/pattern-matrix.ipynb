{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Matrix Replacement\n",
    "\n",
    "This this notebook we explore strategies to replace the ***Hit Correlation*** step of Konrad's pipeline, specifically the *patten matrix* algorithm previously used.\n",
    "\n",
    "## Approach\n",
    "I am thinking of two approaches. First, use a simple MLP to determine if two given points are *causally related* or not. To do this, the following tasks are required:\n",
    "- [x] prepare the data such that each row contains the features (x,y,z,time/timeslice) of each pair of points (exluding self)\n",
    "    - [ ] subtask here is to use PCA to reduce the dimensions and see if the network performs same/worse\n",
    "- [X] create labels for training set ie. *1* of related and *0* otherwise (this hinges upon the fact that we can extract labels from *mc_info* table)\n",
    "- [ ] visualize the results\n",
    "\n",
    "The other approach is to treat this as an unsupervised learning task and use clustering to determine *related* points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from context import km3net\n",
    "from km3net.data import data, pattern_matrix\n",
    "from km3net.utils import DATADIR\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "path = km3net.utils.DATADIR + '/processed/slice-615.csv'\n",
    "df = utils.load(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sample = df.sample(frac=0.1)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the \"Pattern Matrix\" Dataset\n",
    "\n",
    "Since this step will essentially double the width of the dataset and square it's height, we will only use timeslice 665 (timeslice with the largest hits, as per the [exploration](notebooks/exploration.ipynb) conducted previously). The algorithm to generate the dataset is as follows:\n",
    "1. create an empty dataframe to hold the `result`\n",
    "2.iterate over original df with the row and index\n",
    "    1. duplicate original df\n",
    "    2. set value of `dup` columns to that of the row\n",
    "    3. concat dup and original dfs (sideways) to create pairs\n",
    "    4. drop the rows where `id1` is less than `id2` to avoid repeat pairs\n",
    "    5. append dup to result\n",
    "    \n",
    "The algorithm was tested with a small sample of 10 rows before the dataset below was created.\n",
    "\n",
    "## Generate labels\n",
    "\n",
    "Once we \"explode\" the dataset, we need to generate labels. The logic is simple, if `eid1` and `eid2` are same then give it a label of 1, else a label of 0. There are 3 possible combinations that can occur: 1. hit-hit 2. hit-noise and 3. noise-noise. Since noise has `nan` for the event ids and since in Python `nan != nan` all 3 cases can be correctly handled by a simple comparison of the two column values.\n",
    "\n",
    "Drop the columns that are not required for training, and write to csv."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sample.groupby('event_id').count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "exploded_sample = pattern_matrix.process(sample)\n",
    "exploded_sample"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('Shape hits: {0}'.format(exploded_sample[exploded_sample['label'] == 1].shape))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('Shape noise: {0}'.format(exploded_sample[exploded_sample['label'] != 1].shape))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "exploded_sample.to_csv(DATADIR+'/train/slice-615-0-1.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative datasets\n",
    "\n",
    "- [ ] train using the difference between x,y,z,t\n",
    "- [ ] train using a larger sample (50%, 75%, 100% of slice-615)\n",
    "- [ ] train using sample from entire dataset, across timeslices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP for \"Pattern Matrix\" Replacement\n",
    "\n",
    "I followed this [tutorial](https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/) to implement the first iteration of the network.\n",
    "\n",
    "1. what happens if we vary the learning rate and momentum of the optimizer?\n",
    "\n",
    "## Experiment 1\n",
    "\n",
    "### Parameters\n",
    "- Data: 10% of slice 615 (severe class imbalance)\n",
    "- Loss: BCELoss\n",
    "- Optimizer: SGD(lr=0.001, momentum=0.9)\n",
    "- Layers: (8, 10), (10, 8)\n",
    "- Activation: hidden -> ReLu, output -> Sigmoid\n",
    "- Epochs: 10\n",
    "\n",
    "### Accuracy\n",
    "98%\n",
    "\n",
    "### Remarks\n",
    "***This experiment is flawed, since training was done with an imbalanced training set and testing with the same.***\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from context import km3net\n",
    "from km3net.utils import DATADIR\n",
    "import km3net.model.utils as model_utils\n",
    "import km3net.data.utils as data_utils\n",
    "import km3net.data.pattern_matrix as pm\n",
    "from km3net.model.mlp import MLP\n",
    "from torch.nn import BCELoss\n",
    "from torch.optim import SGD\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "path = DATADIR+'/train/slice-615-0-1.csv'\n",
    "train_dl, test_dl = model_utils.prepare_data(path, normalise=True)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "net = MLP(8)\n",
    "optimizer = SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = BCELoss()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_utils.train(train_dl, net, criterion, optimizer, epochs=10)\n",
    "acc = model_utils.test(test_dl, net)\n",
    "print(\"Accuracy: %.3f\" % acc.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2\n",
    "To combat the shortcomings of Experiment 1, we equalize the targets in this experiment whilst keeping the parameters the sames.\n",
    "\n",
    "### Parameters\n",
    "- Data: 10% of slice 615 (equalized classes)\n",
    "- Loss: BCELoss\n",
    "- Optimizer: SGD(lr=0.001, momentum=0.9)\n",
    "- Layers: (8, 10), (10, 8)\n",
    "- Activation: hidden -> ReLu, output -> Sigmoid\n",
    "- Epochs: 10\n",
    "\n",
    "### Accuracy\n",
    "81%\n",
    "\n",
    "### Remarks\n",
    "The model already performs well, but we can improve the performance with perhaps more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context import km3net\n",
    "from km3net.utils import DATADIR\n",
    "import km3net.model.utils as model_utils\n",
    "import km3net.data.utils as data_utils\n",
    "import km3net.data.pattern_matrix as pm\n",
    "from km3net.model.mlp import MLP\n",
    "from torch.nn import BCELoss\n",
    "from torch.optim import SGD\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# data prep cell, run this to generate DATADIR/train/slice-615-0-1.csv\n",
    "\n",
    "headers = ['x1', 'y1', 'z1', 't1', 'x2', 'y2', 'z2', 't2', 'label']\n",
    "df = pd.read_csv(DATADIR+'/train/slice-615-0-1.csv',\n",
    "            \n",
    "\n",
    "     header=None,\n",
    "                names=headers)\n",
    "\n",
    "eqdf = pm.equalise_targets(df)\n",
    "eqdf.to_csv(DATADIR+'/train/slice-615-0-1-equal.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9253 4557\n"
     ]
    }
   ],
   "source": [
    "path = DATADIR+'/train/slice-615-0-1-equal.csv'\n",
    "train_dl, test_dl = model_utils.prepare_data(path,normalise=True)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(8)\n",
    "optimizer = SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.816\n"
     ]
    }
   ],
   "source": [
    "model_utils.train(train_dl, net, criterion, optimizer, epochs=10)\n",
    "acc = model_utils.test(test_dl, net)\n",
    "print(\"Accuracy: %.3f\" % acc.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3\n",
    "Next, we increase the size of training set whilst keeping the parameters the same.\n",
    "\n",
    "### Parameters\n",
    "- Data: 25% of slice 615 (equalized classes)\n",
    "- Loss: BCELoss\n",
    "- Optimizer: SGD(lr=0.001, momentum=0.9)\n",
    "- Layers: (8, 10), (10, 8)\n",
    "- Activation: hidden -> ReLu, output -> Sigmoid\n",
    "- Epochs: 5\n",
    "\n",
    "### Accuracy\n",
    "50%\n",
    "\n",
    "### Remarks\n",
    "Increasing training size does not improve accuracy, problem must be somewhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context import km3net\n",
    "from km3net.utils import DATADIR\n",
    "import km3net.model.utils as model_utils\n",
    "import km3net.data.utils as data_utils\n",
    "import km3net.data.pattern_matrix as pm\n",
    "from km3net.model.mlp import MLP\n",
    "from torch.nn import BCELoss\n",
    "from torch.optim import SGD\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# data prep cell: run this to create 'DATADIR/train/slice-615-2-5.csv'\n",
    "\n",
    "path = DATADIR + '/processed/slice-615.csv'\n",
    "df = pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sample = df.sample(frac=0.25)\n",
    "print(\"Shape hits: {0}\".format(len(sample[sample['label'] == 1])))\n",
    "print(\"Shape noise: {0}\".format(len(sample[sample['label'] == 0])))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "exploded_sample = pm.process(sample)\n",
    "exploded_sample"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# run the above cells until happy with the distribution\n",
    "print('Shape hits: {0}'.format(exploded_sample[exploded_sample['label'] == 1].shape))\n",
    "print('Shape noise: {0}'.format(exploded_sample[exploded_sample['label'] != 1].shape))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "exploded_sample.to_csv(DATADIR+'/train/slice-615-2-5.csv',\n",
    "                       index=False, header=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "path = DATADIR + '/train/slice-615-2-5.csv'\n",
    "df = pd.read_csv(path, header=None, names=pm.col_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "eqdf = pm.equalise_targets(df)\n",
    "eqdf.to_csv(DATADIR+'/train/slice-615-2-5-equal.csv', index=False,\n",
    "            header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42407 20887\n"
     ]
    }
   ],
   "source": [
    "path = DATADIR+'/train/slice-615-2-5-equal.csv'\n",
    "train_dl, test_dl = model_utils.prepare_data(path, normalise=True)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(8)\n",
    "optimizer = SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,  1999] loss: 0.678\n",
      "[1,  1999] loss: 0.562\n",
      "[2,  1999] loss: 0.472\n",
      "[3,  1999] loss: 0.404\n",
      "[4,  1999] loss: 0.324\n",
      "[5,  1999] loss: 0.215\n",
      "[6,  1999] loss: 0.156\n",
      "[7,  1999] loss: 0.130\n",
      "[8,  1999] loss: 0.114\n",
      "[9,  1999] loss: 0.102\n",
      "Accuracy: 0.959\n"
     ]
    }
   ],
   "source": [
    "model_utils.train(train_dl, net, criterion, optimizer, epochs=10)\n",
    "acc = model_utils.test(test_dl, net)\n",
    "print(\"Accuracy: %.3f\" % acc.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
