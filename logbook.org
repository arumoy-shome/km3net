#+TITLE: Km3NeT Logbook
#+AUTHOR: Arumoy Shome
#+DATE: August 20, 2020

* Inbox
  - [2020-08-21 Fri 13:47] [[https://arxiv.org/abs/1206.5533][paper]] on practical recommendations for common
  hyper parameters for gradient descent based models

* Pattern Matrix
  In this section strategies are explored to replace the /Hit
  Correlation/ step of the existing data processing pipeline.
  
** Approach
   Two approaches are considered:
   1. Use a MLP to predict if two given points are /causally related/
      or not.
   2. Approach this problem as an unsupervised learning task and use
      *clustering* techniques to determine related points.

*** MLP to predict /causally related/ points
    + causally related :: two points are causally related to each
      other if they occur close in space and time.

    This approach requires the following sub tasks:
    1. *Preparation of dataset*: Given the "main dataset" a new
       dataset (henceforth referred to as the "pattern matrix
       dataset") is to be created such that each row contains the
       =x,y,z and time= features for all unique pairs of points.
    2. *Creation of labels*: Using the /mc_info/ table, we can
       determine if two points are /related/ if they originated from
       the same "event" ie. they have the same =event_id=.

** Creation of "Pattern Matrix" Dataset
   A sample of the /main/ dataset was choosen as input to the "pattern
   matrix" dataset creation algorithm. Once generated, further random
   samples of varying sizes were considered in order to determine the
   optimal training data size. See [[*Experiments][Experiments]] for details.

   Two sampling variants were explored:
   1. random samples from timeslice 615 only
   2. random samples from mixed timeslices: specifically top 5
      timeslices with the most number of event hits

   The rationale for considering only timeslice 615 being two fold:
   1. It is the timeslice which contains the most number of hits from
      neutrino events
   2. and the fact that the model only needs to learn how to identify
      "related" and "unrelated" hits, simply done by looking at the
      difference of two points. This ofcourse is consistent across
      timeslices thus free of any bias.
   
** Evaluation
   The /main/ dataset is highly skewed, with the *majority* class
   being hits from background noise and the *minority* class being
   Hits from neutrino events. Thus, the /pattern matrix/ dataset is
   also skewed with the *minority* class being related hits and
   *majority* class being unrelated hits.

   While the training dataset contains equal number of samples for
   each class, the testing dataset maintains it's skewed distribution
   since this represents realistic data which the model will be
   required to classify.

   Accuracy is not an ideal metric to use for evaluating the model,
   thus the following alternatives are used:
   1. Recall: this should be high indicating the model is able to
      identify the minority class
   2. Precision: should ideally be high indicating the model does not
      misclassify unrelated hits as related hits, although this is not
      a priority (saving a timeslice with no event hits has less
      weight compared to *not* saving a timeslice containing event hits)
   3. F1 score: should be high, however we care more about the recall
   4. F2 score: since we care more about the recall, we give it more
      weight while calculating the F-beta score
   5. ROC AUC: although this can be misleading since the ROC considers
      both classes and can be over optimistic (due to the skewedness
      of data)
   6. Precision-Recall (PR) AUC: a better alternative to the ROC AUC
      since it focuses on the minority class

   Additionally the ROC curve and the PR curves are also visually
   inspected.

   Relevant sources:
   - [2020-08-20 Thu 21:56] [[https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/][article]] explaining performance metrics for
   imbalanced data
   - [2020-08-21 Fri 11:13] [[https://arxiv.org/pdf/1505.01658.pdf][paper]] presenting an overview of stratergies
     and evaluation techniques for models dealing with highly skewed data
   - [2020-08-21 Fri 11:26] [[https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/][article]] provides a useful flow chart for
     selecting a model evaluation metric when dealing with inbalanced classes

*** Testing datasets
    The following variants were picked to test the model:
    1. no related hits (slice 0): no related hits
    2. low related hits (slice 3650): less than 25 related hits
    3. medium related hits (slice 1800): less than 500 related hits
    4. high related hits (slice 1637): less than 1500 related hits

** DONE Experiments
   :LOGBOOK:
   - State "DONE"       from              [2020-09-04 Fri 14:39]
   :END:
   This section provides a summary of all experiments (and their
   results) which were conducted in order to obtain the final model to
   replace the /pattern matrix/ algorithm of the existing data
   processing pipeline. Details of each experiment can be found in the
   corresponding notebooks in the =notebooks/pm/= directory.

   The final results obtained from each category of experiments are
   summarized below. For further details, the corresponding section
   for the experiment categories follow.

   1. [[*Experiments with dataset][Experiments with dataset]]: 10% random sample from mixed
      timeslices with equal number of samples for each class produced
      the best result.
   2. [[*Experiments with optimizers][Experiments with optimizers]]: No difference was noticed between
      SGD and Adam, =lr\=0.001= produced the best results across
      optimizers.
   3. [[*Experiments with model architecture][Experiments with model architecture]]: two hidden layers with 16
      and 8 neurons respectively produced the best results.

*** Notes on selection of epochs
    The number of epochs is varied per experiment. This is
    because, this parameter is largely determined by the dataset
    itself, and the learning rate of the optimizer.

    In general, the number of epochs reported in each experiment was
    identified by first observing the learning curve and selecting an
    ideal value such that the loss was either reasonably minimized or
    the validation loss did not deteriorate.
*** DONE Experiments with dataset
    :LOGBOOK:
    - State "DONE"       from              [2020-09-04 Fri 12:21]
    :END:
    In these experiments, variants of the data namely it's shape and
    size were manipulated whilst keeping other parameters same. Two
    shape variants were considered:
    1. *original pattern matrix* dataset of shape (n, 9)
    2. and *diff pattern matrix* dataset of shape (n, 5) where the
       difference between the (x,y,z,time) features of the points
       were taken

    Since the dataset is highly skewed, the majority class was
    undersampled for each size variant, which are as follows:
    1. *10%* random sample of slice 615
    2. *25%* random sample of slice 615
    3. *50%* random sample of slice 615
    4. *75%* random sample of slice 615
    5. *10%* random sample of slice mixed

    Overall, diminishing rewards were observed as the size of the
    dataset increased with the *mixed-10-equal-diff* variant producing the
    best results.
**** Summary of results
     The following parameters were constant across all experiments:

     | parameter           | value                                  |
     |---------------------+----------------------------------------|
     | loss                | BCELoss                                |
     | optimizer           | SGD with =lr\=0.001= & =momentum\=0.9= |
     | model architecture  | (inputs, 10) -> (10,8) -> (8, 1)       |
     | activation (hidden) | ReLu                                   |
     | activation (output) | Sigmoid                                |
     | #samples (testing)  | 364231                                 |

     #+begin_example
     The slice-mixed-10-equal-diff dataset produced the best results.
     #+end_example

*** DONE Experiments with optimizers
    :LOGBOOK:
    - State "DONE"       from              [2020-09-04 Fri 12:21]
    :END:
    In this class of experiments, different optimizers were used and
    their /learning rate/ parameter was varied. This is because
    [goodfellow2016deep] suggests that it is the single most important
    hyper parameter.

    The 50%-diff dataset variant was used (since it produced the best
    results in the previous class of experiment, see [[*Experiments with dataset][Experiments with
    dataset]]), all parameters were kept constant whilst /lr/ being
    varied to obtain the final model of the category. See
    =notebooks/pm/exp-optim.ipynb= for more details.

    The different optimizers along with their best results are
    summarized below:
    1. SGD: =lr\=0.001=

    Overall no improvements were noticed.
       
**** Summary of results
     #+begin_example
     No discernable difference were noticed between SGD and Adam.
     Learning rate of 0.001 gave the best results for both optimizers.
     #+end_example

*** DONE Experiments with model architecture
    :LOGBOOK:
    - State "DONE"       from "TODO"       [2020-09-04 Fri 14:35]
    :END:
    In this class of experiments the length and breadth of the model
    are varied. Multiples of 2 were used to determine the number of
    neurons, the minimum being 8 (ie. hidden layer always has a shape
    of =(8, 1)=).

**** Summary of results
     The parameters which gave the best results from the [[*Experiments with dataset][Experiments
     with dataset]] class of experiments were chosen whilst varying the
     length and depth of the model.

     #+begin_example
    The best results were obtained by setting the model architecture
    as =(inputs, 16) -> (16, 8) -> (8,1)= with a recall of 0.81. The
    results were deemed good enough for this model and thus
    experiments for the PM model were concluded.
     #+end_example

** Future Work
   Improvements that can be made to the MLP model.
* Graph Community Detection
  This section describes the strategies explored to replace the /Graph
  Community Detection/ step for the existing data processing pipeline.

** TODO Why we do not need graphConv
   :LOGBOOK:
   - Note taken on [2020-09-11 Fri 16:41] \\
     still pondering over this, we cannot have a perfect MLP so the idea is
     to augment the superior GNN model with edge info from the MLP.
   :END:
   Or, why the graph approach is a different research direction
   altogether.
   Our end goal is : *Given a timeslice, should I save it?*
   If we put 100% faith and trust in the simulated data, and we train
   a neural network to identify hits which are related to each other
   (ie. they originated from the same event). Then, presence of
   related hits above a certain threshold (say 10) directly implies
   that the timeslice is worth saving.

   Graph neural networks on the other hand work on different
   principles. Most relevant to this project would be to do node
   classification which is a semi supervised form of learning. Given
   labels for some of the nodes, the network can predict labels for
   the rest.

   Given that a MLP is much simpler, it should be the preferred over
   Graph Networks.
** Primer on Graph Neural Networks
   It is important to understand the different applications of Graph
   Neural Networks (GNNs) before we proceed. GNNs have two primary
   applications:
   1. *Node classification* which is a semi-supervised learning
      setting. The idea is that given a graph with partial labels, we
      want to conduct label propagation.
   2. *Graph classification* which is a supervised learning setting.
      Here we have several graphs with a corresponding label and we
      want to classify them.

   In this project we use graph classification.

** Approach
   Both node and graph classification can be applied to our problem.
   Let's say we have data for timeslices in the form of =(n, 5)=
   dataframe (5 features because =x, y, z, t and label=).

   For the graph classification approach, each timeslice is a graph
   where each hit (row of dataframe) is represented by a node and all
   nodes are connected by undirected edges. It's label can be obtained
   by looking at the number of event hits present and setting it to 1
   if the count is above a certain threshold. Each node of the graph
   has an embedding/feature vector corresponding to the =(4,)= feature
   vector (row of the dataframe).

   For the node classification approach, the setup is similar however,
   now each node has a label (event or noise) and the training data
   consists of a single graph.

   Further, two approaches can be experimented with:
   1. Train the model with no edge information
   2. Train the model with additional edge information from the PM model

** Experiments
   Two approaches are considered:
   1. Train a model with no edge information
   2. Train a model with additional edge information from the PM model
      
** Data Preparation
   The data prepartion is two fold:
   1. *Prepare data for node classification*: in this case we only
      need a single csv file of =(n,5)= shape where the first 4 column
      are the node features and the last the node label. We require
      another csv of shape =(n**2-n,)= representing the edge weights.
      The csvs must be saved without headers and index.
   2. *Prepare data for graph classification*: this is more involved
      since now we need to create several graphs with a corresponding
      label. The process is the same however we assign the label for
      the graph based on the number of event nodes (if it is above a
      certain threshold N then assign a label of 1 to the graph).

*** Data for Node classification
    This is simple enough, all we do is take a good mix of event and
    noise nodes, concat and save. We do not have to care about
    shuffling or ordering of the rows since we use a fully connected
    graph and the edge weights are derived from the event ids using
    `km3net.data.model.process`.

    We prepare several versions of the training dataset with varying
    number of nodes (100, 500, 1000) but equal classes. The test
    dataset consists of 4 datasets with varying number of examples per
    class.
** Future Work
   Several alternative paths of research are touched upon in this section.

*** CANCELLED Modeling the data as a heterogeneous graph
    :LOGBOOK:
    - State "CANCELLED"  from              [2020-09-11 Fri 16:35] \\
      This may not be feasible for the data we have since different
      entities/node types can be associated with different sets of labels.
    :END:
    Instead of treating nodes of the same entity (ie. a hit), we can
    consider two different kinds: event nodes and noise nodes.

*** Advanced relations from PM model
    The PM model is naive as it only treats event-event pairs from the
    same event as related and the rest as unrelated. This is too
    simplistic since noise-noise pairs are also related and
    event-event pairs from different events are related (since they
    are both event hits) but perhaps to a lesser degree than pairs
    from the same event.

    We can make the PM model a 4 class classifier such that it
    classifying the pair type (event-event-same, event-event-different
    event-noise and noise-noise). This can be further utilized to
    assign varying weights or edge-types to the edges of the graph.

*** Modeling the problem as a graph classification task
    We can construct a graph from a given timeslice and train a model
    to classify it as =SAVE= or =NOSAVE= based on presence of events.
    At the time of writing this, the best approach to create the
    dataset is unclear.

*** Better node and/or edge features
    The node feature and the edge features can be improved such that
    it is more meaningful to the model.

* References
+ [goodfellow2016deep] :: Goodfellow, I., Bengio, Y., Courville, A., &
  Bengio, Y. (2016). Deep learning (Vol. 1). Cambridge: MIT press.
