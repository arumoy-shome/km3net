#+TITLE: Km3NeT Logbook
#+AUTHOR: Arumoy Shome
#+DATE: August 20, 2020

* Inbox
  - [2020-08-21 Fri 13:47] [[https://arxiv.org/abs/1206.5533][paper]] on practical recommendations for common
  hyper parameters for gradient descent based models

* Pattern Matrix
  In this section strategies are explored to replace the /Hit
  Correlation/ step of the existing data processing pipeline.
  
** Approach
   Two approaches are considered:
   1. Use a MLP to predict if two given points are /causally related/
      or not.
   2. Approach this problem as an unsupervised learning task and use
      *clustering* techniques to determine related points.

*** MLP to predict /causally related/ points
    + causally related :: two points are causally related to each
      other if they occur close in space and time.

    This approach requires the following sub tasks:
    1. *Preparation of dataset*: Given the "main dataset" a new
       dataset (henceforth referred to as the "pattern matrix
       dataset") is to be created such that each row contains the
       =x,y,z and time= features for all unique pairs of points.
    2. *Creation of labels*: Using the /mc_info/ table, we can
       determine if two points are /related/ if they originated from
       the same "event" ie. they have the same =event_id=.

** Creation of "Pattern Matrix" Dataset
   A sample of the /main/ dataset was choosen as input to the "pattern
   matrix" dataset creation algorithm. Once generated, further random
   samples of varying sizes were considered in order to determine the
   optimal training data size. See [[*Experiments][Experiments]] for details.

   Two sampling variants were explored:
   1. random samples from timeslice 615 only
   2. random samples from mixed timeslices: specifically top 5
      timeslices with the most number of event hits

   The rationale for considering only timeslice 615 being two fold:
   1. It is the timeslice which contains the most number of hits from
      neutrino events
   2. and the fact that the model only needs to learn how to identify
      "related" and "unrelated" hits, simply done by looking at the
      difference of two points. This ofcourse is consistent across
      timeslices thus free of any bias.
   
** Evaluation
   The /main/ dataset is highly skewed, with the *majority* class
   being hits from background noise and the *minority* class being
   Hits from neutrino events. Thus, the /pattern matrix/ dataset is
   also skewed with the *minority* class being related hits and
   *majority* class being unrelated hits.

   While the training dataset contains equal number of samples for
   each class, the testing dataset maintains it's skewed distribution
   since this represents realistic data which the model will be
   required to classify.

   Accuracy is not an ideal metric to use for evaluating the model,
   thus the following alternatives are used:
   1. Recall: this should be high indicating the model is able to
      identify the minority class
   2. Precision: should ideally be high indicating the model does not
      misclassify unrelated hits as related hits, although this is not
      a priority (saving a timeslice with no event hits has less
      weight compared to *not* saving a timeslice containing event hits)
   3. F1 score: should be high, however we care more about the recall
   4. F2 score: since we care more about the recall, we give it more
      weight while calculating the F-beta score
   5. ROC AUC: although this can be misleading since the ROC considers
      both classes and can be over optimistic (due to the skewedness
      of data)
   6. Precision-Recall (PR) AUC: a better alternative to the ROC AUC
      since it focuses on the minority class

   Additionally the ROC curve and the PR curves are also visually
   inspected.

   Relevant sources:
   - [2020-08-20 Thu 21:56] [[https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/][article]] explaining performance metrics for
   imbalanced data
   - [2020-08-21 Fri 11:13] [[https://arxiv.org/pdf/1505.01658.pdf][paper]] presenting an overview of stratergies
     and evaluation techniques for models dealing with highly skewed data
   - [2020-08-21 Fri 11:26] [[https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/][article]] provides a useful flow chart for
     selecting a model evaluation metric when dealing with inbalanced classes

*** Testing datasets
    The following variants were picked to test the model:
    1. no related hits (slice 0): no related hits
    2. low related hits (slice 3650): less than 25 related hits
    3. medium related hits (slice 1800): less than 500 related hits
    4. high related hits (slice 1637): less than 1500 related hits

** DONE Experiments
   :LOGBOOK:
   - State "DONE"       from              [2020-09-04 Fri 14:39]
   :END:
   This section provides a summary of all experiments (and their
   results) which were conducted in order to obtain the final model to
   replace the /pattern matrix/ algorithm of the existing data
   processing pipeline. Details of each experiment can be found in the
   corresponding notebooks in the =notebooks/pm/= directory.

   The final results obtained from each category of experiments are
   summarized below. For further details, the corresponding section
   for the experiment categories follow.

   1. [[*Experiments with dataset][Experiments with dataset]]: 10% random sample from mixed
      timeslices with equal number of samples for each class produced
      the best result.
   2. [[*Experiments with optimizers][Experiments with optimizers]]: No difference was noticed between
      SGD and Adam, =lr\=0.001= produced the best results across
      optimizers.
   3. [[*Experiments with model architecture][Experiments with model architecture]]: two hidden layers with 16
      and 8 neurons respectively produced the best results.

*** Notes on selection of epochs
    The number of epochs is varied per experiment. This is
    because, this parameter is largely determined by the dataset
    itself, and the learning rate of the optimizer.

    In general, the number of epochs reported in each experiment was
    identified by first observing the learning curve and selecting an
    ideal value such that the loss was either reasonably minimized or
    the validation loss did not deteriorate.
*** DONE Experiments with dataset
    :LOGBOOK:
    - State "DONE"       from              [2020-09-04 Fri 12:21]
    :END:
    In these experiments, variants of the data namely it's shape and
    size were manipulated whilst keeping other parameters same. Two
    shape variants were considered:
    1. *original pattern matrix* dataset of shape (n, 9)
    2. and *diff pattern matrix* dataset of shape (n, 5) where the
       difference between the (x,y,z,time) features of the points
       were taken

    Since the dataset is highly skewed, the majority class was
    undersampled for each size variant, which are as follows:
    1. *10%* random sample of slice 615
    2. *25%* random sample of slice 615
    3. *50%* random sample of slice 615
    4. *75%* random sample of slice 615
    5. *10%* random sample of slice mixed

    Overall, diminishing rewards were observed as the size of the
    dataset increased with the *mixed-10-equal-diff* variant producing the
    best results.
**** Summary of results
     The following parameters were constant across all experiments:

     | parameter           | value                                  |
     |---------------------+----------------------------------------|
     | loss                | BCELoss                                |
     | optimizer           | SGD with =lr\=0.001= & =momentum\=0.9= |
     | model architecture  | (inputs, 10) -> (10,8) -> (8, 1)       |
     | activation (hidden) | ReLu                                   |
     | activation (output) | Sigmoid                                |
     | #samples (testing)  | 364231                                 |

     #+begin_example
     The slice-mixed-10-equal-diff dataset produced the best results.
     #+end_example

*** DONE Experiments with optimizers
    :LOGBOOK:
    - State "DONE"       from              [2020-09-04 Fri 12:21]
    :END:
    In this class of experiments, different optimizers were used and
    their /learning rate/ parameter was varied. This is because
    [goodfellow2016deep] suggests that it is the single most important
    hyper parameter.

    The 50%-diff dataset variant was used (since it produced the best
    results in the previous class of experiment, see [[*Experiments with dataset][Experiments with
    dataset]]), all parameters were kept constant whilst /lr/ being
    varied to obtain the final model of the category. See
    =notebooks/pm/exp-optim.ipynb= for more details.

    The different optimizers along with their best results are
    summarized below:
    1. SGD: =lr\=0.001=

    Overall no improvements were noticed.
       
**** Summary of results
     #+begin_example
     No discernable difference were noticed between SGD and Adam.
     Learning rate of 0.001 gave the best results for both optimizers.
     #+end_example

*** DONE Experiments with model architecture
    :LOGBOOK:
    - State "DONE"       from "TODO"       [2020-09-04 Fri 14:35]
    :END:
    In this class of experiments the length and breadth of the model
    are varied. Multiples of 2 were used to determine the number of
    neurons, the minimum being 8 (ie. hidden layer always has a shape
    of =(8, 1)=).

**** Summary of results
     The parameters which gave the best results from the [[*Experiments with dataset][Experiments
     with dataset]] class of experiments were chosen whilst varying the
     length and depth of the model.

     #+begin_example
    The best results were obtained by setting the model architecture
    as =(inputs, 16) -> (16, 8) -> (8,1)= with a recall of 0.81. The
    results were deemed good enough for this model and thus
    experiments for the PM model were concluded.
     #+end_example

* Graph Community Detection
  This section describes the stratergies explored to replce the /Graph
  Community Detection/ step for the existing data processing pipeline.

** Approach
   The output of the /Pattern Matrix/ model is a one dimensional
   vector consisting binary values {0, 1}. However, it can be "folded"
   back into a (n, n) adjacency matrix (where n is the number of
   samples).

   The adjacency matrix can be converted into a graph which serves as
   the training data for the model.
   
   Graph Convolutional Neural Networks (GconvNN) are used to train on
   graphs created using the output of the /Pattern Matrix/ model. The
   idea being, existance of connected nodes indicate that the
   timeslice contains hits from neutrino events.

* References
+ [goodfellow2016deep] :: Goodfellow, I., Bengio, Y., Courville, A., &
  Bengio, Y. (2016). Deep learning (Vol. 1). Cambridge: MIT press.
