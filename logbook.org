#+TITLE: Km3NeT Logbook
#+AUTHOR: Arumoy Shome
#+DATE: August 20, 2020

* Pattern Matrix
  In this section strategies are explored to replace the /Hit
  Correlation/ step of the existing data processing pipeline.
  
** Approach
   Two approaches are considered:
   1. Use a MLP to predict if two given points are /causally related/
      or not.
   2. Approach this problem as an unsupervised learning task and use
      *clustering* techniques to determine related points.

*** MLP to predict /causally related/ points
    + causally related :: two points are causally related to each
      other if they occur close in space and time.

    This approach requires the following sub tasks:
    1. *Preparation of dataset*: Given the "main dataset" a new
       dataset (henceforth referred to as the "pattern matrix
       dataset") is to be created such that each row contains the
       =x,y,z and time= features for all unique pairs of points.
    2. *Creation of labels*: Using the /mc_info/ table, we can
       determine if two points are /related/ if they originated from
       the same "event" ie. they have the same =event_id=.

** Creation of "Pattern Matrix" Dataset
   A sample of the /main/ dataset, namely all data from the /timeslice
   615/ was choosen as input to the "pattern matrix" dataset creation
   algorithm. Once generated, further random samples of varying sizes
   were considered in order to determine the optimal training data
   size. See [[*Experiments][Experiments]] for details.

   The rationale for considering only timeslice 615 being two fold:
   1. It is the timeslice which contains the most number of hits from
      neutrino events
   2. and the fact that the model only needs to learn how to identify
      "related" and "unrelated" hits, simply done by looking at the
      difference of two points. This ofcourse is consistent across
      timeslices thus free of any bias.
   
** Experiments
   This section provides a summary of all experiments (and their
   results) which were conducted in order to obtain the final model to
   replace the /pattern matrix/ algorithm of the existing data
   processing pipeline. Details of each experiment can be found in
   =notebooks/pattern-matrix.ipynb=.

   The final results obtained from each category of experiments are
   summarized below. For further details, the corresponding section
   for the experiment categories follow.

   1. [[*Experiments with dataset][Experiments with dataset]]: 50% random sample with equal number of
      samples for each class produced the best result.
   2. [[*Experiments with optimizers][Experiments with optimizers]] 
      
*** Notes on selection of epochs
    The number of epochs is varied per experiment. This is
    because, this parameter is largely determined by the dataset
    itself, and the learning rate of the optimizer.

    In general, the number of epochs reported in each experiment was
    identified by first observing the learning curve and selecting an
    ideal value such that the loss was either reasonably minimized or
    the validation loss did not deteriorate.
*** Experiments with dataset
    In these experiments, variants of the data namely it's shape and
    size were manipulated whilst keeping other parameters same. Two
    shape variants were considered:
    1. *original pattern matrix* dataset of shape (n, 9)
    2. and *diff pattern matrix* dataset of shape (n, 5) where the
       difference between the (x,y,z,time) features of the points
       were taken

    Since the dataset is highly skewed, the majority class was
    undersampled for each size variant, which are as follows:
    1. *10%* random sample of slice 615
    2. *25%* random sample of slice 615
    3. *50%* random sample of slice 615
    4. *75%* random sample of slice 615

    Overall, diminishing rewards were observed as the size of the
    dataset increased with the *50%-diff* variant producing the best
    results.
**** Summary of results
     The following parameters were constant across all experiments:

     | parameter           | value                                  |
     |---------------------+----------------------------------------|
     | loss                | BCELoss                                |
     | optimizer           | SGD with =lr\=0.001= & =momentum\=0.9= |
     | model architecture  | (inputs, 10) -> (10,8) -> (8, 1)       |
     | activation (hidden) | ReLu                                   |
     | activation (output) | Sigmoid                                |
     | #samples (testing)  | 364231                                 |


     Following table summarizes the various evaluation metrics observed
     over varying dataset sizes. See [[*Evaluation][Evaluation]] for details on metrics
     used.

     | id |  variant | epochs | #samples (training) | accuracy | recall | precision |   F1 |    F2 | ROCAUC | PRAUC |
     |----+----------+--------+---------------------+----------+--------+-----------+------+-------+--------+-------|
     | 1  |      10% |    100 |                9252 |     0.96 |   0.20 |      0.18 | 0.19 | 0.195 |  0.904 | 0.174 |
     | 1a |      25% |    100 |               42407 |     0.97 |   0.27 |      0.32 | 0.29 | 0.278 |  0.952 | 0.295 |
     | 1b |      50% |    100 |              193898 |     0.97 |   0.34 |      0.30 | 0.32 |  0.33 |   0.95 | 0.283 |
     | 2  | 50%-diff |     10 |              193898 |     0.94 |   0.72 |      0.25 | 0.37 | 0.521 |  0.951 | 0.394 |
     | 2a | 75%-diff |     10 |              407162 |     0.94 |   0.72 |      0.23 | 0.35 | 0.503 |  0.954 | 0.387 |



*** Experiments with optimizers
    In this class of experiments, different optimizers were used and
    their /learning rate/ parameter was varied. This is because
    [goodfellow2016deep] suggests that it is the single most important
    hyper parameter.

    The 50%-diff dataset variant was used (since it produced the best
    results in the previous class of experiment, see [[*Experiments with dataset][Experiments with
    dataset]]), all parameters were kept constant whilst /lr/ being
    varied to obtain the final model of the category. See
    =notebooks/pm/exp-optim.ipynb= for more details.

    The different optimizers along with their best results are
    summarized below:
    1. SGD: =lr\=0.001=

       

**** Summary of results
| id   | variant  |     lr | accuracy | recall | precision |   F1 |    F2 | ROCAUC | PRAUC |
|------+----------+--------+----------+--------+-----------+------+-------+--------+-------|
| base | 50-diff% |  0.001 |     0.94 |   0.72 |      0.25 | 0.37 | 0.521 |  0.951 | 0.394 |
| 1    | 50-diff% | 0.0001 |     0.93 |   0.76 |      0.22 | 0.34 |  0.51 |  0.945 | 0.359 |
| 1a   | 50-diff  |   0.01 |          |        |           |      |       |        |       |

** Evaluation
   The /main/ dataset is highly skewed, with the *majority* class
   being hits from background noise and the *minority* class being
   hits from neutrino events. Thus, the /pattern matrix/ dataset is
   also skewed with the *minority* class being related hits and
   *majority* class being unrelated hits.

   While the training dataset contains equal number of samples for
   each class, the testing dataset maintains it's skewed distribution
   since this represents realistic data which the model will be
   required to classify.

   Accuracy is not an ideal metric to use for evaluating the model,
   thus the following alternatives are used:
   1. Recall: this should be high indicating the model is able to
      identify the minority class
   2. Precision: should ideally be high indicating the model does not
      misclassify unrelated hits as related hits, although this is not
      a priority (saving a timeslice with no event hits has less
      weight compared to *not* saving a timeslice containing event hits)
   3. F1 score: should be high, however we care more about the recall
   4. F2 score: since we care more about the recall, we give it more
      weight while calculating the F-beta score
   5. ROC AUC: although this can be misleading since the ROC considers
      both classes and can be over optimistic (due to the skewedness
      of data)
   6. Precision-Recall (PR) AUC: a better alternative to the ROC AUC
      since it focuses on the minority class

   Additionally the ROC curve and the PR curves are also visually
   inspected.

   Relevant sources:
   - [2020-08-20 Thu 21:56] [[https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/][article]] explaining performance metrics for
   imbalanced data
   - [2020-08-21 Fri 11:13] [[https://arxiv.org/pdf/1505.01658.pdf][paper]] presenting an overview of stratergies
     and evaluation techniques for models dealing with highly skewed data
   - [2020-08-21 Fri 11:26] [[https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/][article]] provides a useful flow chart for
     selecting a model evaluation metric when dealing with inbalanced classes

* Graph Community Detection
  This section describes the stratergies explored to replce the /Graph
  Community Detection/ step for the existing data processing pipeline.

** Approach
   The output of the /Pattern Matrix/ model is a one dimensional
   vector consisting binary values {0, 1}. However, it can be "folded"
   back into a (n, n) adjacency matrix (where n is the number of
   samples).

   The adjacency matrix can be converted into a graph which serves as
   the training data for the model.
   
   Graph Convolutional Neural Networks (GconvNN) are used to train on
   graphs created using the output of the /Pattern Matrix/ model. The
   idea being, existance of connected nodes indicate that the
   timeslice contains hits from neutrino events.

* References
+ [goodfellow2016deep] :: Goodfellow, I., Bengio, Y., Courville, A., &
  Bengio, Y. (2016). Deep learning (Vol. 1). Cambridge: MIT press.
