% this file is called up by thesis.tex
% content in this file will be fed into the main document

\chapter{Replacement for Graph Community Detection Step} % top level followed by section, subsection
\label{cha:gcn}
% ----------------------- contents from here ------------------------
% 

This chapter presents the replacement created using a Graph
Convolutional Neural Networks (GCNs) as proposed by Kipf et al. (2016)
for the \emph{Graph Community Detection Step} of the GPU Pipeline. It
is observed that a GCN is able to identify event nodes very well
however is severely biased to them thus unable to distinguish between
event and noise nodes. The chapter begins with an overview of GCNs and
how they have been applied to this problem. The data preparation and
model evaluation are touched upon next. The chapter concludes with
discussion of the results.

\section{Primer on Graph Convolutional Neural Networks}
\label{sec:gcn-primer}

GCNs are designed to operate on data consisting of entities and their
relations, commonly referred to as \emph{graphs}. A graph $G = (V, E)$
consists of a set \emph{nodes (V)} and a set of \emph{edges (E)}. Each
node may or may not be connected to one or many nodes. These are
referred to as the \emph{neighbors} of the node. A graph with all
nodes connected to one another is called a \emph{fully connected
graph}. An edge may have attributes associated with it, the two most
common attributes being \emph{weight} and \emph{direction}. An edge
$(u, v) \in E$ between two nodes \emph{u} and \emph{v} may be
\emph{directed} which denotes a sense of hierarchy amongst the nodes,
an example being a graph which models how Twitter users follow one
another. An edge may also \emph{undirected} such as a graph which
models the friendship amongst the users of a social network (since
friendship is mutual). An edge may also have a weight to signify a
stronger or weaker connection amongst nodes. Nodes may also posses
attributes associated with themselves, commonly known as \emph{node
embeddings}. The complexity of the node embedding may range from a
simple scalar quantity to a multi-dimensional tensor, and depends on
how the dataset is modeled as a graph.

Graphs are primarily classified into two variants namely
\emph{homogeneous} and \emph{heterogeneous} graphs. Homogeneous graphs
have the same type of entities and relations represented as nodes and
edges respectively. For example, a graph representing the social
network consisting of people and their connections is a homogeneous
graph. In contrast, Heterogeneous graph consist of different types of
nodes and edges. For example, a graph representing a person's likes
and dislikes in regards to food items. Here, two entities, namely
people and food are represented as nodes. The edges also come in two
variants ie. a 'like' and a 'dislike'.

\begin{figure}[htb]
  \centering
  \includegraphics[width=\textwidth]{gcn-message-passing.jpg}
  \caption{Message passing paradigm of GCNs.}
  \label{fig:gcn-message-passing}
\end{figure}
  
GCNs learn by utilizing a message passing paradigm which is summarized
in Figure \ref{fig:gcn-message-passing}, using a fully connected graph
of 5 nodes as example. During each training epoch, all nodes propagate
their embedding to their neighbors (illustrated in sub figure (2)).
The collected embeddings are then aggregated (for example using a sum,
difference or mean) which becomes the new embedding for the node. Sub
figure (3) illustrates the resulting graph structure after the
aggregation procedure. If an embedding is propagated through an edge
carrying a weight then the embedding is scaled by that value. This
procedure is done for all nodes of the graph, for each training epoch.
The number of layers in the network determine how far the messages are
sent. For example, for a network with a single layer, each node
aggregates the embeddings from their immediate neighbors. With 2
layers, the node also aggregates embeddings from the neighbors of its
immediate neighbors and so forth.

\section{Data Preparation}
\label{sec:gcn-data-prep}

\begin{figure}[htb]
  \centering
  \includegraphics[width=\textwidth]{gcn-data.jpg}
  \caption{Overview of GCN dataset creation procedure.}
  \label{fig:gcn-data}
\end{figure}

The graphs for the testing and training of the network are constructed
from a combination of the main dataset and a modified version of the
MLP dataset, Figure \ref{fig:gcn-data} illustrates this procedure. As
observed in Section \ref{sec:mlp-disc} Figure \ref{fig:mlp-cm}, the
MLP model predictions contain false negatives. Using the predictions,
a graph can be constructed such that only related nodes are connected.
This however would lead to the total loss of the event hits which were
incorrectly classified as noise. Thus a fully connected graph is
constructed, and its node embeddings and node labels are derived from
the main dataset. Each node is assigned a $(x,y,z,t)$ vector as its
node embedding. The node is assigned a label of 1 if it is an event
hit, else a label of 0 to denote noise. A modified MLP dataset (see
\ref{sec:mlp-data-prep}) with a shape of \texttt{$(n^{2}-n, 5)$} is
created such that each hit is paired with all other hits except
itself. The label column from this dataset is then used as the edge
weights of the graph. Edges between event nodes from the same event
thus are assigned a weight of 1 and all other edges are assigned a
weight of 0.

Since the main dataset and the MLP dataset are highly skewed, the GCN
dataset is also skewed with majority of the nodes being noise. Similar
strategy as used in the creation of the MLP training set (see
\ref{sec:mlp-data-prep}) is used. The training set is a graph with
approximately 1000 nodes equally distributed amongst the classes. The
skewed nature of the data is maintained in the testing sets. The model
is evaluated with 3 test sets, each with varying levels of event
nodes. In practise, the pipeline will observe timeslices with no to
very few events, thus the performance of the model on test set 1 and 2
should be given importance. The various test sets and their
distribution are summarized in Table \ref{tab:gcn-test-dist}.

\begin{table}[htb]
  \centering
  \caption{Distribution of GCN testing datasets.}
  \begin{tabular}{lrrr}
    \hline
    & Total examples & Positive examples & Negative examples \\
    \hline
    \textbf{TS1} & 1000-1500 & -- & 1000-1500 \\
    \textbf{TS2} & 1000-1500 & 10-20 & 990-1480 \\
    \textbf{TS3} & 1000-1500 & 200-250 & 800-1250 \\
    \hline
  \end{tabular}
  \label{tab:gcn-test-dist}
\end{table}

\section{Model Description and Evaluation}
\label{sec:gcn-model-desc-eval}

\begin{table}[htb]
  \centering
  \caption{GCN Model Parameter Summary.}
  \begin{tabular}{lr}
    \hline
    Loss & BCELoss \\
    Optimizer & Adam with learning rate of $0.001$ \\
    Hidden Activation & ReLu \\
    Output Activation & Sigmoid \\
    \hline
  \end{tabular}
  \label{tab:gcn-model-param}
\end{table}

The model is expected to classify nodes of an unseen graph as event or
noise nodes. Since causally related nodes are connected with edges
carrying a high weight, the model is expected to group them together
resulting in a final graph with separate clusters of causally related
nodes and noise nodes. The parameters of the model are summarized in
Table \ref{tab:gcn-model-param}. The rational for selecting the
parameters remains the same as that of the MLP model (see
\ref{sec:mlp-model-desc}) since both models perform binary
classification. The difference comes from the model architecture which
is summarized in Table \ref{tab:gcn-model-arch}. The GCN model
comprises of an input layer, two graph convolutional layers and an
output layer. The network is fully connected with 4 neurons in the
input layer, 16 in both graph convolutional layers and 1 neuron in the
output layer. A dropout layer is added between the two Gconv layers to
prevent overfitting \cite{srivastava14dropout}. The same evaluation
metrics are used (see Section \ref{sec:mlp-model-eval}) since the GCN
dataset is also highly skewed in nature.

\begin{table}
  \centering
  \caption{GCN model architecture summary.}
  \begin{tabular}{lrrrr}
    \hline
    Layer position & Type & Activation & In features & Out features \\
    \hline
    \textbf{1} & GConv & ReLU & 4 & 16 \\
    \textbf{2} & dropout & -- & -- \\
    \textbf{3} & GConv & RelU & 16 & 2 \\
    \textbf{4} & Linear & Sigmoid & 2 & 1 \\
    \hline
  \end{tabular}
  \label{tab:gcn-model-arch}
\end{table}

\section{Results}
\label{sec:gcd-disc}

\begin{figure}[htb]
  \centering
    \includegraphics[width=0.5\textwidth]{gcn-learning.jpg}
    \caption{Learning Curve for GCN.}
  \label{fig:gcn-learning}
\end{figure}

\begin{table}[htb]
  \centering
  \caption{Summary of GCN performance across test sets.}
  \begin{tabular}{lrrrrrrr}
    \hline
    & Accuracy & Precision & Recall & F1 & F2 & ROCAUC & PRAUC \\
    \hline
    \textbf{TS1} & 0.52 & -- & -- & -- & -- & -- & -- \\
    \textbf{TS2} & 0.58 & 0.04 & 1.00 & 0.08 & 0.18 & 0.87 & 0.06 \\
    \textbf{TS3} & 0.67 & 0.40 & 1.00 & 0.57 & 0.77 & 0.81 & 0.36 \\
    \hline
  \end{tabular}
  \label{tab:gcn-results}
\end{table}

A good fit is achieved by the model during training as seen in Figure
\ref{fig:gcn-learning}. In addition to the model's performance on the
various test sets as summarized by Table \ref{tab:gcn-results}, the
node embedding of the training and testing graphs are also inspected
using t-SNE \cite{maaten2008visualizing}. Figure
\ref{fig:gcn-train-tsne} shows the node embedding of the training
before and after training. It is interesting to note that the model is
able to learn as clusters of similar nodes are noticed after training.

\begin{figure}[htb]
  \centering
  \subfloat[Before training.]{\includegraphics[width=0.5\textwidth]{gcd-train-tsne.png}}
  \subfloat[After training.]{\includegraphics[width=0.5\textwidth]{gcd-train-tsne-after.png}}
  \caption{TSNE for GCN training dataset.}
  \label{fig:gcn-train-tsne}
\end{figure}

Inspecting Figure \ref{fig:gcn-test-tsne} one can see the model's
inability to cluster similar nodes in the testing sets. Very scarce
communities are observed in TS1 and TS2 with the model only being able
to cluster the event nodes in TS3. The model is biased to the minority
class as is seen in the Confusion Matrices depicted in Figure
\ref{fig:gcn-cm}. The model is able to identify all event nodes in TS2 and
TS3 perfectly however has a high number of FPs in all 3 test sets.
This indicates that the presence of the high edge weights assigned to
causally related nodes greatly aids the model in identifying the event
nodes. However, since all other edges are assigned a weight of 0, the
model does not quite learn to identify the noise nodes.

\begin{figure}[htb]
  \centering
  \subfloat[CM for TS1.]{\includegraphics[width=0.33\textwidth]{gcd-cm-no.png}}
  \subfloat[CM for TS2.]{\includegraphics[width=0.33\textwidth]{gcd-cm-medium.png}}
  \subfloat[CM for TS3.]{\includegraphics[width=0.33\textwidth]{gcd-cm-high.png}}
  \caption{CM for GCN Test Datasets.}
  \label{fig:gcn-cm}
\end{figure}

\begin{figure}[htb]
  \centering
  \subfloat[TS1 before.]{\includegraphics[width=0.33\textwidth]{gcd-test-no-tsne.png}}
  \subfloat[TS2 before.]{\includegraphics[width=0.33\textwidth]{gcd-test-medium-tsne.png}}
  \subfloat[TS3 before.]{\includegraphics[width=0.33\textwidth]{gcd-test-high-tsne.png}}

  \subfloat[TS1 after.]{\includegraphics[width=0.33\textwidth]{gcd-test-no-tsne-after.png}}
  \subfloat[TS2 after.]{\includegraphics[width=0.33\textwidth]{gcd-test-medium-tsne-after.png}}
  \subfloat[TS3 after.]{\includegraphics[width=0.33\textwidth]{gcd-test-high-tsne-after.png}}
  \caption{TSNE for GCN Test Datasets with naive edge weights.}
  \label{fig:gcn-test-tsne}
\end{figure}

% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------
