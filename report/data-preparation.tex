% this file is called up by thesis.tex
% content in this file will be fed into the main document

\chapter{Data Preparation} % top level followed by section, subsection
\label{cha:data-prep}

% ----------------------- contents from here ------------------------

At the time of undertaking this project, the KM3NeT Neutrino Telescope was
still under construction, thus  simulated data provided by Nikhef was used for
the project. The data itself was split onto two parts namely \emph{events} and
\emph{noise}, both of which came from different sources and in different
formats.

\section{Preparation of \emph{events} dataset}%
\label{sec:data-prep-events}
The \emph{events} dataset was provided as a \emph{HDF5} (Hierarchical Data
Format) with a size of 42MB consisting of the \texttt{/data/mc\_hits} and
\texttt{/data/mc\_info} tables. For the purposes of this project, the two
tables were combined such that each row in the \textit{mc\_hits} table contains
it's corresponding 'event\_id' from the \textit{mc\_info}
table. A \texttt{label} column was added containing a value of '1' and the
resulting table (henceforth referred to as the \emph{events} dataset) was saved as
a CSV file for future use.

\section{Preparation of \emph{noise} dataset}%
\label{sec:data-prep-noise}
The \emph{noise} data was generated using a Python library written and
maintained by Nikhef, \texttt{k40gen}.
\texttt{k40gen.Generators(21341, 1245, [7000., 700., 70., 0.])} was
used to create an instance of a generator where the first two
arguments are random seeds followed by a list of rates at which
single, double, triple and quadruple hits should be generated. The
generator instance is then passed into \texttt{k40gen.generate\_40()}
method which returns a (4, n) array containing (time (t), dom\_id,
pmt\_id, time over threshold (tot)). The position coordinates (ie.
\texttt{x}, \texttt{y}, \texttt{z} coordinates) for each datapoint was
provided in a \emph{positions.detx} file which was parsed using the
Numpy Python package \cite{numpy} and added to the \emph{noise} array.
The Python library Pandas \cite{pandas} was used to convert the array
into a (n, 4) dataframe. A \texttt{label} column was added containing
a value of '0' and the dataframe was saved as a 3.9GB CSV file.

\section{Preparation of \emph{main} dataset}%
\label{sec:data-prep-main}

To create the \emph{main} dataset for the project, the \emph{events} and
\emph{noise} datasets were combined. Both datasets were read into memory as
Pandas dataframes and their columns were renamed consistently. The two
dataframes were concatenated and sorted based on the \texttt{time} column. Rows
with a negative \texttt{time} value were dropped along with columns which are
not relevant to this project. The \texttt{time} column was discretized into
15000ns bins and the resulting values were added to the \texttt{timeslice}
column. The resulting dataframe was saved as a 1.9GB CSV file.

% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------
